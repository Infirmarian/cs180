\documentclass[titlepage]{article}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{listings}
\title{CS 180 Homework 1}
\author{Robert Geil \\
University of California, Los Angeles
}
\lstset{frame=tb,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  tabsize=3
}
\numberwithin{equation}{subsection}
\begin{document}
\maketitle

\section{Television Network Programming}
\subsection{Problem}
Two television networks are competing for programming slots for a season,
and each attempts to maximize the number of shows that are selected. Each
network has $n$ shows, and there are $n$ slots of programming. A given network
\textit{wins} a slot if their proposed show has a higher ranking than their
competitor. Is there a stable matching for any two sets of shows? We define
\textit{stable} as a state where one network cannot unilaterally change their
schedule to win more slots. For example, if the networks provide schedules $A$ and $B$,
but if the first network changed its schedule to $A'$ and as a result won more slots,
$A$ and $B$ would be an unstable schedule.
\subsection{Counter Example}
There is no way for any two $n$ show lists to be formed into a stable matching.
For example, consider the simple counter case of Network A, who has 2 shows of ranking
1 and 3, and Network B, who has shows of ranking 2 and 4. If A submits schedule
(1, 3) and B submits (2, 4), A wins 0 slots. However, A could unilaterally 
change their schedule to (3, 1) and then win 1 slot (against B's (2, 4)). In
response, B could then switch to (4, 2) and win back their two slot advantage. However,
A could then switch the ordering of their two shows as well, bringing us to an endless
back-and-forth, which does not meet the definition of stable.
\section{National Resident Matching Program}
\subsection{Problem}
A group of $m$ hospitals wants to hire a number of residents from $n$ medical
school graduates. Each hospital ranks the students in terms of priority (1 - $n$) and
each student ranks the hospitals in terms of priority (1 - $m$). We must create
an algorithm that avoids the instability where student $s$ is matched with hospital $h$,
$s'$ is not matched, but $h$ prefered $s'$ to $s$. In addition, the instability where there exist
students $s$ and $s'$, and hospitals $h$ and $h'$, and $s$ is matched with $h$, $s'$ is with $h'$, but
$h$ was higher on $s'$'s list, and $s'$ was higher on $h$'s list must be avoided.
\subsection{Algorithm}
\begin{minipage}{\linewidth}
    \begin{lstlisting}
    while there are hospitals with slots not filled:
        pick an arbitrary hospital with a slot, and ask their top choice (who they haven't asked yet) to come to the hospital
        if the student hasn't been selected for any hospital yet, accept
        otherwise:
            if the student's already selected hospital has a higher ranking than the asking hospital, break off with the current hospital
            otherwise reject the asking hospital
    \end{lstlisting}
\end{minipage}
\subsection{Proof of No Instability}
\subsubsection{Unmatched Student Instability}
Assume there are arbitrary students $s$ and $s'$, and hospital $h$. $s$ is matched with $h$, and $h'$ has
no hospital, but $h$ prefers $s'$ to $s$. This implies that there hospital $h$ requested $s$ before
$s'$, as by the algorithm, any student who is paired with a hospital will always either remain with
that hospital \textbf{or} go to another hospital. However, since $h$ prefers $s'$ to $s$, following
the algorithm, $h$ would have asked $s'$ before $s$, so we have a contradiction.
\subsubsection{Unstable Hospital Student Match}
Assume there are arbitrary students $s$ and $s'$, and hospitals $h$ and $h'$. $s$ ranks $h'$ above $h$, and
$h'$ ranks $s$ above $s'$. In order for $s$ and $h$ to be matched, there must have been a hospital $h' < h''$
which requested $s$, and then to be paired with $h$, $h'' < h$. However, this forms a contradiction, as by the
transitive property of inequality, $s$ must prefer $h$ to $h'$, but we have assumed that $s$ ranks $h'$ above $h$, 
therefore this could not have occurred using this algorithm
\subsection{Runtime}
Each hospital may ask every student on its list before a matching is found, but since a hospital never asks
a student multiple times, they will ask at most $n$ times. With $m$ hospitals each completing these $n$ possible
requests, the worst-case runtime of this algorithm is O($nm$).
\section{Shipping Maintainence}
\subsection{Problem}
A shipping company has \textit{n} ships, and \textit{n} ports. Each ship
visits all \textit{n} ports in a month that is \textit{m} days long, where
$m>n$. In addition, no two ships can be in a port on the same day, and each
ship truncates its regular shipping schedule to perform maintainence before
the end of the month.
\subsection{Algorithm}
This problem has many parallels to the marriage problem. The problem set can
be represented by a bipartite graph, with one side being the ships, and one the
ports. There can be only 1 \textit{final docking} between any given ship and port,
and there is a \textit{priority} for both ships and ports, in the form of the 
scheduled order that the ships are to dock. In order to perform the matching of when to truncate
a ship's itinerary, follow these steps
\begin{enumerate}
    \item For each ship, create a \textit{n} long list with the numbers of its ports in the \textit{reverse} schedule order (ie last visited port is first on the list)
    \item For each port, create a \textit{n} long list with then numbers of its ship schedule in the \textit{reverse} order
    \item Check if there is 1 or more un-truncated ship, if not, exit
    \item Arbitrarily choose an un-truncated ship, and try to end-dock with the first port on it's list that hasn't been requested to dock yet. If that port doesn't already have a ship for maintainence, schedule the asking ship in and go to step 3.
    \item If the requesting ship has an earlier scheduled docking date with the port, reject the match, otherwise cancel the prior ship-port docking in favor of this new one. Go to step 3
\end{enumerate}
Just as in the marriage problem, at the completion of the algorithm, each port will be matched with exactly one ship, with the 
optimizing factor being how late in the month the ship docks for the remainder of the month.
\subsection{Proof of the Existance of a Solution}
Assume there are two arbitrary ports $p$ and $p'$, and two arbitrary ships $s$ and $s'$.
For a solution to not exist, there would have to be a choice where $s$ docks with $p$ after $s'$ has already
visited $p'$ and before visiting $p$. However this forms a contradiction,
because if $s'$ planned to visit $p'$ before visiting $p$, there must be some port $p''$ such that
$p'$ is earlier than $p''$, and $p$ is earlier than $p''$ in $s'$'s schedule. However, this is a contradiction, so
it is proven that in the scenario of two ports and two ships, there cannot be a matching that doesn't
work out
\subsection{Efficiency}
The runtime of this algorithm is O($n^2$), as every ship (\textit{n}) may in the worst case have to
set up or query a docking with every port \textit{n}. Because a given port cannot be queried by
a single ship multiple times, the solution is not worse than a runtime of O($n^2$)
\section{Ordering Runtimes}
Provided with the equations
\begin{enumerate}
    \item $g_1(n) = 2^{\sqrt{log(n)}}$
    \item $g_2(n) = 2^n$
    \item $g_3(n) = n^{4/3}$
    \item $g_4(n) = n(log(n))^3$
    \item $g_5(n) = n^{log(n)}$
    \item $g_6(n) = 2^{2^n}$
    \item $g_7(n) = 2^{n^2}$
\end{enumerate}

\subsection{Equation Ordering}
Let us begin with Equation 1. This can be reduced using the log power rule to
$2^{log(n)/2}$. Since the base of the log is constant, the log can be "de-expontiated" to
$dn$, where \textit{d} is a constant.
\subsubsection{Equation 4}
    Equation 4 is strictly greater than Equation 1
    \begin{equation}
        n \leq n(log(n))^3
    \end{equation}
    Dividing both sides by \textit{n} (assuming \textit{n} strictly positive) gives
    \begin{equation}
        1 \leq log(n)^3
    \end{equation}
    which is asymptotically true
\subsubsection{Equation 3}
    Equation 4 can be rewritten using Log Power rules to 
    \begin{equation}
        3nlog(n)
    \end{equation}
    While Equation 3 can be written as
    \begin{equation}
        n(log(n)) \leq n*n^{1/3}
    \end{equation}
    Dividing by \textit{n} (assuming that \textit{n} is positive) yields 
    \begin{equation}
        log(n) \leq n^{1/3}
    \end{equation}
    As per the textbook, polynomial growth is greater than log growth, so this 
    inequality is asymptotically true
\subsubsection{Equation 5}
    Equation 5 grows faster than Equation 3 because the exponent \textit{log(n)} is
    non-constant, as compared to \textit{4/3}, and therefore grows faster, so
    \begin{equation}
        n^{4/3} \leq n^{log(n)}
    \end{equation}
\subsubsection{Equation 2}
    Equation 2 is an exponential equation.
    By the definitions in the textbook, we can see that for asymptotic behavior,
    exponential equations are greater than polynomial equations, so it must hold that
    \begin{equation}
        n^{log(n)} \leq 2^n
    \end{equation}
\subsubsection{Equations 6 and 7}
    By using the Power Law, equations 6 and 7 can both be reduced to the equivelent
    \begin{equation}
        4^n
    \end{equation}
    Since 4 is strictly greater than 2, we see that equations 6 and 7 increase
    faster than equation 2
\subsection{Final Ordering}
Our series of proofs, through the law of syllogism, prove that the increasing 
ordering of O(\textit{n}) runtimes should be 
\textbf{1, 4, 3, 5, 2, 6, 7} or
\begin{equation}
    O(n) \leq O(nlog(n)) \leq O(n^{4/3}) \leq O(n^{log(n)}) \leq  O(2^n) \leq O(4^n)
\end{equation}

\section{Proofs By Induction}
\subsection{Prove (by induction) that sum of 
the first n integers (1 + 2 + .... + \textit{n}) is \textit{n}(\textit{n} + 1)/2 }
\subsubsection{Base Case}
\begin{equation}
    n = 1
\end{equation}
so the original equation, when substituting \textit{n} trivially becomes
\begin{equation}
    1 = 1(1 + 1) / 2
\end{equation}
by simple evaluation of this equation in the case \textit{n} = 1, we can assure ourselves
that the equality holds. Therefore, we prove the base case

\subsubsection{Inductive Step}
Assume that
\begin{equation}
    1 + 2 + ... + k = k(k + 1)/ 2
\end{equation}
We will then prove the \textit{k+1}st case
\begin{equation}
    1 + 2 + ... + k + (k+1) = (k+1)((k+1)+1)/2
\end{equation}
\begin{equation}
    ... = (k+1)(k+2)/2
\end{equation}
Distributing the $k+1$ gives
\begin{equation}
    ... = (k(k+1) + 2(k+1))/2
\end{equation}
\begin{equation}
    ... = k(k+1)/2 + 2(k+1)/2
\end{equation}
\begin{equation}
    1 + 2 + ... + k + (k+1) = k(k+1)/2 + (k+1)
\end{equation}
Subtracting k+1 from both sides gives us the equation
\begin{equation}
    1 + 2 + ... + k = k(k+1)/2
\end{equation}
which by our earlier assumption is true, therefore we prove
that given the \textit{k}th case is true, the \textit{k+1}st case also holds

\subsection{What is 1\textsuperscript{3} +2\textsuperscript{3} + 3\textsuperscript{3} +...+ n\textsuperscript{3} = ?? Prove your answer by induction.}
Based on a brief inspection of the first few cases, it appears the pattern follows the square of the \textit{triangular numbers},
i.e. 1\textsuperscript{2}, 3\textsuperscript{2}, 6\textsuperscript{2}, 10\textsuperscript{2}, 15\textsuperscript{2}, \dots\\If that is the case, the equation which matches this is
\begin{equation}
    (n(n+1)/2)^2
\end{equation}
We will now prove by induction that this is true for all \textit{n} $\geq$ 1
\subsubsection{Base Case (\textit{n=1})}
\begin{equation}
    1^3 = (1(1+1)/2)^2
\end{equation}
Simply evaluating the subsitiution of the base case, we seen that the
equation reduces down to the true statement \textit{1 = 1}
\subsubsection{Inductive Step}
We assume that the equation holds for \textit{n = k}, i.e.
\begin{equation}
    1^3 + 2^3 + \dots + k^3 = (k(k+1)/2)^2
\end{equation}
Now we seek to prove the \textit{k+1}st case
\begin{equation}
    1^3 + 2^3 + \dots + k^3 + (k+1)^3 = ((k+1)((k+1)+1)/2)^2
\end{equation}
the right hand side then simplifies to
\begin{equation}
    \dots = ((k+1)(k+2)/2)^2
\end{equation}
Distributing the \textit{k+2} and the division by two gives us
\begin{equation}
    \dots = (k(k+1)/2+(k+1))^2
\end{equation}
evaluating the square then provides
\begin{equation}
    \dots = (k(k+1)/2)^2 + k(k+1)^2 + (k+1)^2
\end{equation}
from there, a $(k+1)^2$ can be factored out from
the right two terms, giving
\begin{equation}
    1^3 + 2^3 + \dots + k^3 + (k+1)^3 = (k(k+1)/2)^2 + (k+1)(k+1)^2
\end{equation}
by subtracting $(k+1)^3$ from both sides, we
reach the original assumed Kth case, therefore proving that the sum of
the first \textit{n} cubes is $(k(k+1)/2)^2$

\section{Egg Drop Problem}
\subsection{Description}
In the \textit{Egg Drop Problem}, there is a ladder with a number of rungs. An egg will
break if it is dropped from an unknown rung, \textit{k}. In order to find \textit{k}, two (2) 
eggs are provided, which may be dropped repeatedly until they break. If an egg is broken,
it can no longer be used. One goal of this problem is optimization, miniminzing the number
of times an egg must be dropped
\subsection{Algorithm}
In an absolute minimum case, there must be two drops of the egg. Once at the maximum height
from which the egg will \textbf{not} break, and one from the next rung on the ladder up, showing
that it \textbf{will} break. Since two eggs are provided, there is a bit
more flexibility when it comes to dropping, as the first egg can be used
to probe in a non-linear fashion, and followed by the second egg in a linear
way once the first has broken. To follow this algorithm:
\begin{enumerate}
    \item Drop the egg from the middle rung of the ladder
    \item If it breaks, linear probe from the lowest tested rung (if the egg is broken on the first drop, the bottom) until the second egg breaks
    \item Otherwise, go up half the distance to the top of the ladder and drop the first egg again, and go to step 2
\end{enumerate}
\subsection{Runtime}
\subsubsection{200 Rung Ladder}
If the ladder has 200 rungs, in the worst case using this algorithm 100 drops will be performed.
\textit{k = 100} is the worst case. Here, we drop the egg from halfway up (step 100) and it breaks.
We then have to perform 99 more drops (from steps 1-99), all of which succeed in order to confirm that
the highest step the egg won't break on is step 99. 
\subsubsection{\textit{n} Rung Ladder}
Similarly, if the ladder has \textit{n} rungs, the worst case is \textit{n}/2 drops, if the highest non-breaking
rung is \textit{n}/2 - 1.
\subsubsection{Big-O Runtime}
The efficiency of this algorithm is O(\textit{n}), where \textit{n} is the number of
rungs in the ladder. This is because in the worst case \textit{n}/2 drops must be performed,
which is equivelent to a constant times \textit{n}, or O(\textit{n})
\end{document}