\documentclass[titlepage]{article}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{listings}
\title{CS 180 Homework 4}
\author{Robert Geil \\
University of California, Los Angeles
}
\lstset{frame=tb,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  tabsize=2
}
\numberwithin{equation}{subsection}
\begin{document}
\maketitle
\section{Weighted Job Ordering}
\subsection{Problem}
A company has a series of jobs presented to it, each with a completion time
and a weight denoting the importance. The value of a given job $i$ completed is
weight($i$)*start\_time($i$). The company is seeking a way to minimize the total
weighted sum of all $n$ jobs presented.
\subsection{Algorithm}
The key to this algorithm is accounting for both the time of a job and the priority
of the job. Jobs with a high time should be scheduled later, but jobs with a high
priority should be scheduled earlier
\begin{lstlisting}
for each job j:
    assign a score to j calculated by priority(j)/time(j)
sort the jobs based on their scores
perform jobs in reverse ordering (highest score first)
\end{lstlisting}
\subsection{Proof}
Assume there is some optimal ordering $O$ which minimizes the total cost (as defined in the 
problem statment) of executing the jobs. We will prove that our proposed algorithm $P$ performs at least
as well as this optimal algorithm. In order to perform this proof, we will observe the list in a reverse order.
We can see that there is some number $i$ such that the last $i$ elements of $O$ and $P$ are the same. 
This can be assumed as the value of $i$ must only constrain to $i\geq 0$. As such, we observe the 
$i+1$ element (or 1 further element from the end of the job sequence). Here we see that our algorithm
will select the smallest ratio of the unassigned jobs. This means that every other possibility would have
had a larger-than or equal-to weight:time ratio. By swapping our next selected job with the one picked by the
optimal solution $O$, it is therefore not possible to make a swap between the two algorithms next choices
which causes our solution to take less time or greater weight. As this is true for the base case and the
inductive step, we prove there is no swapping of tasks between our algorithm and the optimal one which
improves times, therefore proving that our algorithm is at least as good as an optimal implementation.
\subsection{Runtime}
The primary driver in this algorithm is the sorting based on ratio of priority to time. As with general
sorting problems, this can be done in O($n\log n$) time. From there, the algorithm just relies on a linear
traversal through the list, contributing just O($n$) time. This gives an overall time complexity of O($n\log n$).
\section{Round-The-Clock Scheduling}
\subsection{Problem}
A CPU is provided with a list of daily tasks to complete, each of which has a start time and a stop time.
Provide an algorithm that can efficiently take this list of $n$ tasks and schedule them so the most number
of tasks can be completed. Tasks can strech across a day-break period, such as from 11:00pm-1:00am
\subsection{Algorithm}
The major difference between this and the regular Interval Scheduling Problem is where to begin the
algorithm. With the traditional problem, the beginning is easily found, while with this modified solution,
there is no such concept of an \textit{earliest ending} task.
\begin{lstlisting}
sort the n jobs based on ending time to a list L
for each job j:
    count the number of overlapping jobs
select the job with the lowest number of overlapping jobs, and accept that job
set the starting time s to the start time of the first accepted job
set the ending time e to the end time of the first accepted job
for each job j in L:
    if j starts after e and j ends before s:
        accept j
        set e to j's end time
\end{lstlisting}
\subsection{Proof}
This algorithm leads to an optimal solution. The biggest difference between this and the traditional
Interval Scheduling Problem is the lack of a starting point. Our algorithm selects this first item to
add to the list by choosing the job with the fewest overlapping other jobs. Since the number of overlapping
jobs is the number of jobs elimated from acceptance, we prove that selecting the lowest of these numbers will
give the most effective choice for maintaining the largest number of remaining potential jobs. Once this initial
point is selected, the remaining accepted jobs are choosen through the traditional algorithm. The proof of
the Interval Scheduling Problem uses a proof by induction. Since the algorithm chooses the earliest possible
end time, there is no possibility to switch between any given step and the optimal solution and provide a shorter
total time. Since the optimal scheduler and the given \textit{choose-earliest-ending} algorithm are both the same
for the first $i\geq0$ steps, and are the same for any given step, they are proven to be the same. Therefore, by
use of a proven best selection of starting point, and a best algorithm for the non-circular case, we prove that
our algorithm is optimal for finding the largest number of jobs on a circular interval.
\subsection{Runtime} %TODO: Ensure that the runtime for finding overlapping elements is correct
The runtime of sorting is O($n\log n$). Finding the number of overlapping jobs can be done in linear
time with a sorted list, and going through the remaining $n-1$ elements to either accept or reject the job
is also O($n$). Overall, this gives us a runtime of O($n\log n$).
\section{Banking Fraud}
\subsection{Problem}
A banking company is attempting to find whether at least $n/2$ of the cards in a given group are belonging
to the same bank account. Cards can only be tested pairwise for equivalence, i.e. checking to see 
if cards $i$ and $j$ are part of the same account. Provide an algorithm that can do this with at most
O($n\log n$) comparisons.
\subsection{Algorithm}
This problem is fundamentally one of finding a \textit{majority} among a group. In order to solve this problem,
we can use a "majority candidate" which is found with a single pass through the list, and confirmed with a
second pass through the list
\begin{minipage}{\linewidth}
\begin{lstlisting}
let mc be the majority candidate (initially null)
let c be the count for the majority
for each element e in the group:
    if mc is null:
        set mc to e
    else:
        compare mc and e
        if mc equals e:
            increment c
        else:
            decrement c
            if c = 0:
                set mc to be null
if mc is null:
    there is no majority
otherwise:
    make a counter c
    for each element e (besides mc):
        compare e and mc
        if e = mc:
            increment c
    if c > n/2:
        there is a majority
    otherwise:
        there is no majority
\end{lstlisting}
\end{minipage}
\subsection{Proof}
This algorithm relies on the fact that if a majority exists among a group, removing two distinct elements
will never destroy the majority. We can see that to have a majority, there must be at least $n/2+1$ of the
majority (by definition), and a total of $n$ in the group. Consider the cases of removing two from separate groups
\subsubsection{Removing neither from the majority}
Doing this, we reduce the number of the group to $n-2$, while the majority still has $n/2+1$. If $n/2+1$ is a
majority of $n$, it is also a majority of $n-2$.
\subsubsection{Removing one from the majority}
By removing one from the majority, the new majority value is $n/2$, while the new total is $n-2$. Given the new
total is $n-2$ and the fact that a majority is greater than 50\%, to have a majority of the new total, one
must have more than $(n-2)/2 = n/2 - 1$. We see that our new majority value is indeed greater than this. Therefore
we prove that removing different values will not destroy the majority. Using this principle, we proceed through
our first pass of the list. If two nodes are the same, we increment the count of the majority candidate. Otherwise,
we reduce the count of the majority candidate. Different nodes is equivelent to removing two nodes of a different type.
At the end, we have the only card that could possibly be more than $n/2$ of the total set. Going through again
all the elements and now counting the number that match with the candidate. If more than $n/2$ match with the
candidate, then there is a majority. Otherwise, none.
\subsection{Runtime}
Overall the runtime on this algorithm is O($n$). Since each node is checked just once against the running
candidate, and then again against the final candidate, there is a linear number of card comparisons
done.
\section{Local Minimum on a Grid Graph}
\subsection{Problem}
There exists an $n$x$n$ graph $G$ such that the only edges on a given node are those
which would be directly above, below, left and right on a chessboard (at a distance of exactly 1).
Each of the $n^2$ nodes is labeled with a distinct real number. Find a local minimum (a node such that
the 4 surrounding nodes all have a label greater than the minimum) in O($n$) time.
\subsection{Algorithm}
\begin{minipage}{\linewidth}
\begin{lstlisting}
while G is not empty:
    select a line of nodes (horizontal or vertical) L such that L splits G roughly in half. Slices must alternate horizontal and vertical
    find the minimum node m in L with a linear search
    if L is horizontal:
        if the values above and below m are greater than m:
            m is a local minimum
        else if the value above m is greater than the value below m:
            discard the "top half" of G
        else:
            discard the "bottom half" of G
    if L is vertical:
        if the values left and right of m are greater than m:
            m is a local minimum
        else if the value left of m is greater than the value right of m:
            discard the "left half" of G
        else:
            discard the "right half" of G
\end{lstlisting}
\end{minipage}
\subsection{Proof}
This algorithm relies on a few principles. First, it must be true that a global minimum is also
a local minimum. By definition, a global minimum is less than all other values, and by extension
must also be less than the 4 surrounding values. Second, this proof relies on finding the minimum of
a line. Within a line, the minimum value must also be a local minimum, by a similar reasoning to the case
of a global minimum in a grid. In addition, we see that in any subset of the original graph $G$, there must
be a global minimum of the subset, and such a minimum is also a local minimum, unless a smaller value appears
on the removed border with the rest of the graph. Using the above principles, we can prove that our algorithm
will always find a local minimum. Selecting a divide gives us a slice of length $n$, and finding the minimum
on that gives a potential local minimum. In both cases, we are guaranteed that the minimum value is a local
min within the line it is selected from.
\subsubsection{Case 1}
The selected node's 4 neighbors are all less than it. In this case, we have successfully found a local minimum.
\subsubsection{Case 2}
If one (or both) of the non-inline neighbors of the selected node are less than it, we have some knowledge about
one (or both) of the sides of the line. Assume an arbitrary neighbor (left, right, up, or down, depending on
the orientation of the line) is less than selected node. Because of this, we know by extension that that neighbor
is less than all elements of the slice, as the selected node is a minimum of the slice. Since there is a subset global
minimum in any subset of the graph, we know that there is also a subset in the neighbor's side of the graph. 
Since the neighbor is less than all values on the slice, the global subset minimum must also be less than
all values on the slice. Therefore, by the above assertion, we know that a local minimum must exist within the
smaller neighbor's subset of the graph. Taking this step recursively (with no loss of generality) proves that
the algorithm will always find a local minimum.
\subsection{Runtime}
The runtime analysis on this algorithm is a bit more complicated than others. We see that it follows a
divide-and-conquer algorithm style, and because roughly half the graph is discarded at each step, there will
be at most $\log n$ steps. However, the amount of work done at each step is not constant. At each step, a slice
of the graph is linearly probed for a minimum value, initially taking O($n$) time. However, since we have the constraint
that each slice switches between vertical and horizontal, each time the length of the slice is reduced by half.
This means that we end up with the sequence
\begin{equation}
    n + \frac{n}{2} + \frac{n}{4} + \frac{n}{8} + \dots
\end{equation}
factoring out $n$ gives the converging summation
\begin{equation}
    \sum_{i=0}^{\infty} \frac{1}{2^i} = 2
\end{equation}
Since $\log n$ steps is less than infinity, and by multiplying back in $n$, we can see that the runtime is 
$\leq 2n$, which is equal to O($n$).
\section{Shifted Sorted Numbers}
\subsection{Problem}
You are provided an array of $n$ sorted integers that have been circularly shifted by $k$ spots to the
right. Design an efficient (better then linear time) algorithm to find $k$
\subsection{Algorithm}
This solution follows a similar pattern to binary search, splitting the list each time and discarding half of it.\\
\begin{minipage}{\linewidth}
\begin{lstlisting}
set a value low to the first array entry
set a value high to the last array entry
while low is not equal to high:
    select the midpoint m between low and high
    if m >= low:
        set low to m
    if m < low:
        set high to m
the index of low (and high) is the number offset
\end{lstlisting}
\end{minipage}
\subsection{Proof}
In an unshifted list, it should be that the values are strictly non-decreasing from the first to the last
position. With the shift, there is some point within the list such that the value at position $i$ 
is greater than $i+1$. We will prove that this algorithm finds that position by cases.
\subsubsection{Case 1: midpoint $\geq$ low}
This case agrees with our prior assumption about the unshifted list, and since we know that the values between
all points except the shifted index are increasing, it must be true that all values between midpoint and low
are increasing, so the shift cannot have occurred in the first half of the list. Therefore that portion of the
list may be discarded.
\subsubsection{Case 2: midpoint $<$ low}
In this case, there is some issue (ie the offset) in the first half of the list, since in a strictly non-decreasing
list, it would be impossible for the low value to be greater than the midpoint. This implies that the shift
occurred somewhere between the mid and low points, and therefore the top half of the list my be discarded.
By looking at these two cases, we prove that we never throw out the portion of the list that contains
the shift, and since this process continues until the effective length of the list is 1, it must converge
to the offset.
\subsection{Runtime}
Because this algorithm discards half the list each iteration, and proceeds until the list is of size 1, there
are $\log n$ iterations of the loop. Within the loop, a constant time operation of comparing two values is done,
giving us the overall runtime of O($\log n$).
\section{Heap Implementation}
\subsection{Problem}
Describe how a heap data structure can be maintained and used to extract the minimum, insert a value,
and change a value. Analyze the time complexity for each of these three operations.
\subsection{Min Heap}
In order to maintain the minimum element within easy access, we will implement a \textit{Min Heap}.
This data structure will follow the heap principles. Namely, it is a complete tree structure where the
leaves of any node are always greater than the parent. We will use an array to store the data, and allow the
equations for the locations of the children to be used to navigate the tree.
\begin{equation}
\text{index}(\text{child}_1) = 2*\text{index}(\text{parent}) + 1
\end{equation}
\begin{equation}
\text{index}(\text{child}_2) = 2*\text{index}(\text{parent}) + 2
\end{equation}
Correspondingly, this gives us easy access to the index of the parent of a node
\begin{equation}
\text{index}(\text{parent}) = \lfloor \frac{\text{index}(\text{child})-1}{2} \rfloor
\end{equation}
Within the array, we may see something like the following sample Min Heap
\begin{center}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    1 & 5 & 3 & 9 & 7 & 4 &  \\
    \hline
    \end{tabular}
    \end{center}
Above we see the heap with 1 as the minimum value, and 1 having two children, 5 and 3.
5 also has two children, 9 and 7, while 3 has 1 child, 4. This tree follows the heap property
that each of the two children nodes of a parent are greater than the parent.
\subsection{Extract Min}
The operation to get the minimum is very easy, simply look at the first element of the array. Because the
heap property guarantees us that the children of this node are greater than it, and recursively the children
of those two nodes are greater than it, we know that the smallest value exists at the first position of the
tree. Once it has been extracted, however, how do you determine which value to replace it with? It must be
one of the two children of the extracted node, as these are the next two smallest values in the tree
\begin{minipage}{\linewidth}
\begin{lstlisting}
set a node x point to the root
while x has two children:
    select the smaller of x's children, and set x's value to the value of this child
    set x to point to that child
if x has 1 child:
    set x's value to the value of the child
    delete the child
if x has no children:
    delete x
\end{lstlisting}
\end{minipage}
This algorithm will traverse down the tree, swapping the minimum of the two children with the current value,
and then proceeding to focus on that child, until a child occurrs with 1 or zero children, in which case either
the child (with the value copied up) will be deleted (zero children), or the single child will be swapped up
and the dangling node deleted. We can see that this maintains the heap property, as at each level, since the
parent is swapped with the smaller of its children, the heap property remains intact. 
\subsection{Insert}
In order to insert a value and maintain the heap property, the new value needs to be placed into the heap
in such a way that it is less than its two children. This can be acomplished by a \textit{trickle up} operation.
\begin{lstlisting}
insert a new node k at the end of the array
while k has a parent (isn't the root) and k's parent is greater than k:
    swap k and its parent
\end{lstlisting}
This simple algorithm will fill in a new node $k$ in the last spot available, corresponding to the 
bottom of the tree. This may break the heap property, however, and therefore must be corrected. The property
states that a child cannot have a higher value than its parent. To achieve this, we swap the new node and its
parent, trickling upward until either the new node is the root or its parent is less than it.
\subsection{Change A Number}
To change a number, we can combine a couple of the previous algorithms. Rather than changing the value in place,
we can delete the old value and then insert the new value to achieve the same effect. First we can perform the 
\textit{extract min} operation on the given node, which will remove it from the heap. Then we perform our
\textit{insert} operation with the new value. This effectively changes the value of the node.
\subsection{Runtime}
The runtime of each of these three algorithms is O($n\log n$). For the extract-min algorithm, it takes
constant time to remove the head of the list. Shuffling up can take in the worst case time in relation to
the number of levels on the tree. Since a heap is by definition a full tree, there are at most $\log n$ levels, 
giving us a time complexity of O($\log n$). For insert, keeping track of the end of the array can be done,
allowing us to insert to the end in constant time. The \textit{reheapify} process of shuffling the new value
upwards may have to visit at most $\log n$ levels, since it may have to go all the way up to the root if the
new value inserted is a new minimum. This gives us O($\log n$). Finally, for the changing a number, we make an
assumption that we have a pointer to the number to be changed. If so, we can combine the runtimes of the delete
and insert operations (O($\log n$) + O($\log n$) = O($\log n$)). However, if the value needs to be found,
we must perform a linear search through the entire tree, which takes O($n$). Therefore the runtime of the
\textit{change-value} operation takes either O($\log n$) or O($n$) time, depending on if a pointer to the
node to be changed is known.
\end{document}