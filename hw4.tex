\documentclass[titlepage]{article}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{listings}
\title{CS 180 Homework 4}
\author{Robert Geil \\
University of California, Los Angeles
}
\lstset{frame=tb,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  breaklines=true,
  tabsize=2
}
\numberwithin{equation}{subsection}
\begin{document}
\maketitle
\section{Weighted Job Ordering}
\subsection{Problem}
A company has a series of jobs presented to it, each with a completion time
and a weight denoting the importance. The value of a given job $i$ completed is
weight($i$)*start\_time($i$). The company is seeking a way to minimize the total
weighted sum of all $n$ jobs presented.
\subsection{Algorithm}
The key to this algorithm is accounting for both the time of a job and the priority
of the job. Jobs with a high time should be scheduled later, but jobs with a high
priority should be scheduled earlier
\begin{lstlisting}
for each job j:
    assign a score to j calculated by priority(j)/time(j)
sort the jobs based on their scores
perform jobs in reverse ordering (highest score first)
\end{lstlisting}
\subsection{Proof}
Assume there is some optimal ordering $O$ which minimizes the total cost (as defined in the 
problem statment) of executing the jobs. We will prove that our proposed algorithm $P$ performs at least
as well as this optimal algorithm. In order to perform this proof, we will observe the list in a reverse order.
We can see that there is some number $i$ such that the last $i$ elements of $O$ and $P$ are the same. 
This can be assumed as the value of $i$ must only constrain to $i\geq 0$. As such, we observe the 
$i+1$ element (or 1 further element from the end of the job sequence). Here we see that our algorithm
will select the smallest ratio of the unassigned jobs. This means that every other possibility would have
had a larger-than or equal-to weight:time ratio. By swapping our next selected job with the one picked by the
optimal solution $O$, it is therefore not possible to make a swap between the two algorithms next choices
which causes our solution to take less time or greater weight. As this is true for the base case and the
inductive step, we prove there is no swapping of tasks between our algorithm and the optimal one which
improves times, therefore proving that our algorithm is at least as good as an optimal implementation.
\subsection{Runtime}
The primary driver in this algorithm is the sorting based on ratio of priority to time. As with general
sorting problems, this can be done in O($n\log n$) time. From there, the algorithm just relies on a linear
traversal through the list, contributing just O($n$) time. This gives an overall time complexity of O($n\log n$).
\section{Round-The-Clock Scheduling}
\subsection{Problem}
A CPU is provided with a list of daily tasks to complete, each of which has a start time and a stop time.
Provide an algorithm that can efficiently take this list of $n$ tasks and schedule them so the most number
of tasks can be completed. Tasks can strech across a day-break period, such as from 11:00pm-1:00am
\subsection{Algorithm}
The major difference between this and the regular Interval Scheduling Problem is where to begin the
algorithm. With the traditional problem, the beginning is easily found, while with this modified solution,
there is no such concept of an \textit{earliest ending} task.
\begin{lstlisting}
sort the n jobs based on ending time to a list L
for each job j:
    count the number of overlapping jobs
select the job with the lowest number of overlapping jobs, and accept that job
set the starting time s to the start time of the first accepted job
set the ending time e to the end time of the first accepted job
for each job j in L:
    if j starts after e and j ends before s:
        accept j
        set e to j's end time
\end{lstlisting}
\subsection{Proof}
This algorithm leads to an optimal solution. The biggest difference between this and the traditional
Interval Scheduling Problem is the lack of a starting point. Our algorithm selects this first item to
add to the list by choosing the job with the fewest overlapping other jobs. Since the number of overlapping
jobs is the number of jobs elimated from acceptance, we prove that selecting the lowest of these numbers will
give the most effective choice for maintaining the largest number of remaining potential jobs. Once this initial
point is selected, the remaining accepted jobs are choosen through the traditional algorithm. The proof of
the Interval Scheduling Problem uses a proof by induction. Since the algorithm chooses the earliest possible
end time, there is no possibility to switch between any given step and the optimal solution and provide a shorter
total time. Since the optimal scheduler and the given \textit{choose-earliest-ending} algorithm are both the same
for the first $i\geq0$ steps, and are the same for any given step, they are proven to be the same. Therefore, by
use of a proven best selection of starting point, and a best algorithm for the non-circular case, we prove that
our algorithm is optimal for finding the largest number of jobs on a circular interval.
\subsection{Runtime} %TODO: Ensure that the runtime for finding overlapping elements is correct
The runtime of sorting is O($n\log n$). Finding the number of overlapping jobs can be done in linear
time with a sorted list, and going through the remaining $n-1$ elements to either accept or reject the job
is also O($n$). Overall, this gives us a runtime of O($n\log n$).
\section{Banking Fraud}
\subsection{Problem}
A banking company is attempting to find whether at least $n/2$ of the cards in a given group are belonging
to the same bank account. Cards can only be tested pairwise for equivalence, i.e. checking to see 
if cards $i$ and $j$ are part of the same account. Provide an algorithm that can do this with at most
O($n\log n$) comparisons.
\subsection{Algorithm}
This problem is fundamentally one of grouping, which can be done in a divide-and-conquer format.
At the lowest level, each card can be placed in a group by itself, and then each comparison either merges two
groups together or keeps them seperate.
% TODO:
\begin{lstlisting}
assign each of the n items a group
arbitrarily pair groups together
for each pair of groups:
    if the first element of the both groups is equal:
\end{lstlisting}
\section{Local Minimum on a Grid Graph}
\subsection{Problem}
There exists an $n$x$n$ graph $G$ such that the only edges on a given node are those
which would be directly above, below, left and right on a chessboard (at a distance of exactly 1).
Each of the $n^2$ nodes is labeled with a distinct real number. Find a local minimum (a node such that
the 4 surrounding nodes all have a label greater than the minimum) in O($n$) time.
\subsection{Algorithm}
\begin{minipage}{\linewidth}
\begin{lstlisting}
while G is not empty:
    select a line of nodes (horizontal or vertical) L such that L splits G roughly in half. Slices must alternate horizontal and vertical
    find the minimum node m in L with a linear search
    if L is horizontal:
        if the values above and below m are greater than m:
            m is a local minimum
        else if the value above m is greater than the value below m:
            discard the "top half" of G
        else:
            discard the "bottom half" of G
    if L is vertical:
        if the values left and right of m are greater than m:
            m is a local minimum
        else if the value left of m is greater than the value right of m:
            discard the "left half" of G
        else:
            discard the "right half" of G
\end{lstlisting}
\end{minipage}
\subsection{Proof}
This algorithm relies on a few principles. First, it must be true that a global minimum is also
a local minimum. By definition, a global minimum is less than all other values, and by extension
must also be less than the 4 surrounding values. Second, this proof relies on finding the minimum of
a line. Within a line, the minimum value must also be a local minimum, by a similar reasoning to the case
of a global minimum in a grid. In addition, we see that in any subset of the original graph $G$, there must
be a global minimum of the subset, and such a minimum is also a local minimum, unless a smaller value appears
on the removed border with the rest of the graph. Using the above principles, we can prove that our algorithm
will always find a local minimum. Selecting a divide gives us a slice of length $n$, and finding the minimum
on that gives a potential local minimum. In both cases, we are guaranteed that the minimum value is a local
min within the line it is selected from.
\subsubsection{Case 1}
The selected node's 4 neighbors are all less than it. In this case, we have successfully found a local minimum.
\subsubsection{Case 2}
If one (or both) of the non-inline neighbors of the selected node are less than it, we have some knowledge about
one (or both) of the sides of the line. Assume an arbitrary neighbor (left, right, up, or down, depending on
the orientation of the line) is less than selected node. Because of this, we know by extension that that neighbor
is less than all elements of the slice, as the selected node is a minimum of the slice. Since there is a subset global
minimum in any subset of the graph, we know that there is also a subset in the neighbor's side of the graph. 
Since the neighbor is less than all values on the slice, the global subset minimum must also be less than
all values on the slice. Therefore, by the above assertion, we know that a local minimum must exist within the
smaller neighbor's subset of the graph. Taking this step recursively (with no loss of generality) proves that
the algorithm will always find a local minimum.
\subsection{Runtime}
The runtime analysis on this algorithm is a bit more complicated than others. We see that it follows a
divide-and-conquer algorithm style, and because roughly half the graph is discarded at each step, there will
be at most $\log n$ steps. However, the amount of work done at each step is not constant. At each step, a slice
of the graph is linearly probed for a minimum value, initially taking O($n$) time. However, since we have the constraint
that each slice switches between vertical and horizontal, each time the length of the slice is reduced by half.
This means that we end up with the sequence
\begin{equation}
    n + \frac{n}{2} + \frac{n}{4} + \frac{n}{8} + \dots
\end{equation}
factoring out $n$ gives the converging summation
\begin{equation}
    \sum_{i=0}^{\infty} \frac{1}{2^i} = 2
\end{equation}
Since $\log n$ steps is less than infinity, and by multiplying back in $n$, we can see that the runtime is 
$\leq 2n$, which is equal to O($n$).
\section{Shifted Sorted Numbers}
\subsection{Problem}
You are provided an array of $n$ sorted integers that have been circularly shifted by $k$ spots to the
right. Design an efficient (better then linear time) algorithm to find $k$
\subsection{Algorithm}
This solution follows a similar pattern to binary search, splitting the list each time and discarding half of it.
\begin{lstlisting}
set a value low to the first array entry
set a value high to the last array entry
while low is not equal to high:
    select the midpoint m between low and high
    if m >= low:
        set low to m
    if m < low:
        set high to m
the index of low (and high) is the number offset
\end{lstlisting}
\subsection{Proof}
In an unshifted list, it should be that the values are strictly non-decreasing from the first to the last
position. With the shift, there is some point within the list such that the value at position $i$ 
is greater than $i+1$. We will prove that this algorithm finds that position by cases.
\subsubsection{Case 1: midpoint $\geq$ low}
This case agrees with our prior assumption about the unshifted list, and since we know that the values between
all points except the shifted index are increasing, it must be true that all values between midpoint and low
are increasing, so the shift cannot have occurred in the first half of the list. Therefore that portion of the
list may be discarded.
\subsubsection{Case 2: midpoint $<$ low}
In this case, there is some issue (ie the offset) in the first half of the list, since in a strictly non-decreasing
list, it would be impossible for the low value to be greater than the midpoint. This implies that the shift
occurred somewhere between the mid and low points, and therefore the top half of the list my be discarded.
By looking at these two cases, we prove that we never throw out the portion of the list that contains
the shift, and since this process continues until the effective length of the list is 1, it must converge
to the offset.
\subsection{Runtime}
Because this algorithm discards half the list each iteration, and proceeds until the list is of size 1, there
are $\log n$ iterations of the loop. Within the loop, a constant time operation of comparing two values is done,
giving us the overall runtime of O($\log n$).
\end{document}